# Propuesta de Mapeo Mejorado para Modelos Compat

## AnÃ¡lisis de Modelos Disponibles

### Proveedores
- **mks** (Ollama): 81 modelos
- **localai** (OpenAI-compatible): 37 modelos

### Modelos por CaracterÃ­sticas

#### Chat General (con razonamiento)
Los modelos Qwen3 TODOS tienen capacidad de razonamiento segÃºn la DB:
- `qwen3:4b` - 32K context, reasoning âœ“
- `qwen3:8b` - 32K context, reasoning âœ“
- `qwen3:14b` - 32K context, reasoning âœ“
- `qwen3:30b` - 32K context, reasoning âœ“
- `qwen3:32b` - 32K context, reasoning âœ“

Otros modelos de chat:
- `llama3.2:3b` - 128K context
- `llama3.1:8b` - 128K context
- `llama3.3:70b` - 8K context
- `mistral:7b` - 32K context
- `mistral-small:22b` - 32K context

#### Modelos de Razonamiento Dedicados
- `deepseek-r1:7b` - reasoning âœ“
- `deepseek-r1:8b` - reasoning âœ“
- `deepseek-r1:14b` - reasoning âœ“
- `deepseek-r1:32b` - reasoning âœ“
- `gpt-oss:20b` - reasoning âœ“
- `magistral:24b` - reasoning âœ“ (mistral con razonamiento)
- `qwq:32b` - modelo de razonamiento especializado

#### Modelos Multimodales (Vision + Razonamiento)
**Los modelos Qwen3-VL tienen VISION + REASONING:**
- `qwen3-vl:4b` - 32K context, vision âœ“, reasoning âœ“
- `qwen3-vl:8b` - 32K context, vision âœ“, reasoning âœ“
- `qwen3-vl:30b` - 32K context, vision âœ“, reasoning âœ“
- `qwen3-vl:32b` - 32K context, vision âœ“, reasoning âœ“

Otros modelos de visiÃ³n:
- `llama3.2-vision:11b` - 128K context, vision âœ“
- `gemma3:12b` - 8K context, vision âœ“
- `mistral-small3.1:24b` - 32K context, vision âœ“
- `mistral-small3.2:24b` - 32K context, vision âœ“
- `qwen2.5vl:7b` - 32K context, vision âœ“

#### CÃ³digo
- `qwen2.5-coder:1.5b` - 32K context
- `qwen2.5-coder:7b` - 32K context
- `qwen2.5-coder:14b` - 32K context
- `qwen3-coder:30b` - 32K context

#### Embeddings
- `qwen3-embedding:0.6b` - 32K context
- `qwen3-embedding:8b` - 32K context
- `nomic-embed-text`
- `bge-large`
- `bge-m3`
- `mxbai-embed-large`

---

## ðŸ”¥ PROPUESTA DE MAPEO MEJORADO

### 1. CHAT MODELS (Progressive Scale)

#### `gpt-3.5-turbo` â†’ **`qwen3:4b`** (proveedor: `mks`)
**Cambio:** De `qwen3:4b` â†’ sigue igual âœ“
- Contexto: 32K (suficiente para GPT-3.5)
- Razonamiento: âœ“
- Uso: Fast, general chat

#### `gpt-3.5-turbo-16k` â†’ **`qwen3:8b`** (proveedor: `mks`)
**Cambio:** De `qwen3:4b` â†’ **`qwen3:8b`** (mÃ¡s apropiado para 16K)
- Contexto: 32K
- Razonamiento: âœ“
- MÃ¡s capacidad que el modelo base

#### `gpt-4o-mini` â†’ **`qwen3:8b`** (proveedor: `mks`)
**Cambio:** Sigue igual âœ“
- Contexto: 32K
- Razonamiento: âœ“
- Balanced performance

#### `gpt-4` â†’ **`qwen3:32b`** (proveedor: `mks`)
**Cambio:** De `gpt-oss:20b` â†’ **`qwen3:32b`**
**RazÃ³n:** Qwen3:32b tiene razonamiento nativo Y mÃ¡s parÃ¡metros
- Contexto: 32K
- Razonamiento: âœ“
- Premium quality

#### `gpt-4-32k` â†’ **`llama3.1:8b`** (proveedor: `mks`)
**Cambio:** De `gpt-oss:20b` â†’ **`llama3.1:8b`**
**RazÃ³n:** El nombre enfatiza contexto grande (128K > 32K)
- Contexto: 128K âœ“âœ“âœ“
- Large context capability

#### `gpt-4o` â†’ **`qwen3:32b`** (proveedor: `mks`)
**Cambio:** Sigue igual âœ“
- Contexto: 32K
- Razonamiento: âœ“
- Premium model

#### **NUEVO:** `gpt-4-turbo` â†’ **`llama3.3:70b`** (proveedor: `mks`)
**RazÃ³n:** El modelo mÃ¡s grande y potente disponible
- Contexto: 8K (suficiente para la mayorÃ­a de casos)
- ParÃ¡metros: 70B (mÃ¡xima calidad)
- Best quality available

---

### 2. VISION MODELS

#### `gpt-4-vision-preview` â†’ **`llama3.2-vision:11b`** (proveedor: `mks`)
**Cambio:** Sigue igual âœ“
- Contexto: 128K âœ“âœ“
- VisiÃ³n: âœ“
- Estable y bien probado

#### `gpt-4-turbo-vision` â†’ **`qwen3-vl:8b`** (proveedor: `mks`)
**Cambio:** De `qwen3-vl:8b` â†’ sigue igual âœ“
**VENTAJA ESPECIAL:** Vision + Reasoning!
- Contexto: 32K
- VisiÃ³n: âœ“
- Razonamiento: âœ“ (Ãºnico!)

#### `gpt-4o-vision` â†’ **`qwen3-vl:32b`** (proveedor: `mks`)
**Cambio:** Sigue igual âœ“
**VENTAJA ESPECIAL:** Vision + Reasoning en modelo grande!
- Contexto: 32K
- VisiÃ³n: âœ“
- Razonamiento: âœ“
- Premium quality

#### **NUEVO:** `gpt-4-vision-mini` â†’ **`qwen3-vl:4b`** (proveedor: `mks`)
**RazÃ³n:** OpciÃ³n rÃ¡pida para visiÃ³n
- Contexto: 32K
- VisiÃ³n: âœ“
- Razonamiento: âœ“
- Fast processing

---

### 3. EMBEDDING MODELS

#### `text-embedding-3-small` â†’ **`nomic-embed-text`** (proveedor: `mks`)
**Cambio:** Sigue igual âœ“
- Lightweight, fast
- Bien optimizado

#### `text-embedding-ada-002` â†’ **`nomic-embed-text`** (proveedor: `mks`)
**Cambio:** Sigue igual âœ“
- Legacy compatibility

#### `text-embedding-3-large` â†’ **`qwen3-embedding:8b`** (proveedor: `mks`)
**Cambio:** Sigue igual âœ“
- Contexto: 32K
- High quality
- Large dimensions (4096)

---

### 4. REASONING MODELS

#### `o1-mini` â†’ **`deepseek-r1:7b`** (proveedor: `mks`)
**Cambio:** Sigue igual âœ“
- Razonamiento: âœ“
- Fast, lightweight

#### `o1-preview` â†’ **`deepseek-r1:14b`** (proveedor: `mks`)
**Cambio:** Sigue igual âœ“
- Razonamiento: âœ“
- Balanced

#### `o1` â†’ **`deepseek-r1:32b`** (proveedor: `mks`)
**Cambio:** Sigue igual âœ“
- Razonamiento: âœ“
- Premium reasoning

#### **NUEVO:** `o3-mini` â†’ **`deepseek-r1:8b`** (proveedor: `mks`)
**RazÃ³n:** OpciÃ³n intermedia entre o1-mini y o1-preview
- Razonamiento: âœ“
- Balanced performance

#### **NUEVO:** `o1-pro` â†’ **`qwq:32b`** (proveedor: `mks`)
**RazÃ³n:** Modelo especializado en razonamiento complejo
- Razonamiento avanzado
- 32B parÃ¡metros

---

### 5. CODE MODELS

#### `code-davinci-002` â†’ **`qwen2.5-coder:14b`** (proveedor: `mks`)
**Cambio:** Sigue igual âœ“
- Contexto: 32K
- Code specialized

#### `gpt-4-code` â†’ **`qwen3-coder:30b`** (proveedor: `mks`)
**Cambio:** Sigue igual âœ“
- Contexto: 32K
- Premium code model

#### **NUEVO:** `gpt-3.5-turbo-instruct` â†’ **`qwen2.5-coder:7b`** (proveedor: `mks`)
**RazÃ³n:** Modelo rÃ¡pido para code completion
- Contexto: 32K
- Fast code generation

---

## ðŸ“Š RESUMEN DE CAMBIOS

### Cambios Principales
1. **`gpt-4`**: `gpt-oss:20b` â†’ **`qwen3:32b`** (mejor razonamiento, mÃ¡s parÃ¡metros)
2. **`gpt-4-32k`**: `gpt-oss:20b` â†’ **`llama3.1:8b`** (128K contexto)
3. **`gpt-3.5-turbo-16k`**: `qwen3:4b` â†’ **`qwen3:8b`** (mÃ¡s apropiado)

### Modelos Nuevos Agregados
1. **`gpt-4-turbo`** â†’ `llama3.3:70b` (mÃ¡xima calidad)
2. **`gpt-4-vision-mini`** â†’ `qwen3-vl:4b` (visiÃ³n rÃ¡pida)
3. **`o3-mini`** â†’ `deepseek-r1:8b` (razonamiento intermedio)
4. **`o1-pro`** â†’ `qwq:32b` (razonamiento premium)
5. **`gpt-3.5-turbo-instruct`** â†’ `qwen2.5-coder:7b` (code completion)

### Ventajas Clave
- âœ“ Uso de modelos con **razonamiento nativo** (qwen3 series)
- âœ“ Modelos **multimodales Ãºnicos** (qwen3-vl con vision + reasoning)
- âœ“ Mejor escalado por **tamaÃ±o y capacidades**
- âœ“ Aprovechamiento de **contextos largos** (llama3 128K)
- âœ“ EspecializaciÃ³n apropiada (embeddings, code, reasoning, vision)

---

## ðŸŽ¯ MAPEO RECOMENDADO POR CASO DE USO

| Caso de Uso | Modelo OpenAI | Modelo Ollama | CaracterÃ­sticas |
|-------------|---------------|---------------|-----------------|
| Chat rÃ¡pido | gpt-3.5-turbo | qwen3:4b | 32K ctx, reasoning |
| Chat balanceado | gpt-4o-mini | qwen3:8b | 32K ctx, reasoning |
| Chat premium | gpt-4, gpt-4o | qwen3:32b | 32K ctx, reasoning |
| MÃ¡xima calidad | gpt-4-turbo | llama3.3:70b | 70B params |
| Contexto largo | gpt-4-32k | llama3.1:8b | 128K ctx |
| VisiÃ³n estable | gpt-4-vision-preview | llama3.2-vision:11b | 128K ctx, vision |
| VisiÃ³n + razonamiento | gpt-4-turbo-vision | qwen3-vl:8b | vision + reasoning |
| VisiÃ³n premium | gpt-4o-vision | qwen3-vl:32b | vision + reasoning |
| VisiÃ³n rÃ¡pida | gpt-4-vision-mini | qwen3-vl:4b | vision + reasoning |
| Embeddings rÃ¡pidos | text-embedding-3-small | nomic-embed-text | lightweight |
| Embeddings quality | text-embedding-3-large | qwen3-embedding:8b | 32K ctx, 4096 dim |
| Razonamiento rÃ¡pido | o1-mini | deepseek-r1:7b | reasoning |
| Razonamiento balanceado | o1-preview | deepseek-r1:14b | reasoning |
| Razonamiento intermedio | o3-mini | deepseek-r1:8b | reasoning |
| Razonamiento premium | o1 | deepseek-r1:32b | reasoning |
| Razonamiento avanzado | o1-pro | qwq:32b | advanced reasoning |
| CÃ³digo rÃ¡pido | gpt-3.5-turbo-instruct | qwen2.5-coder:7b | code |
| CÃ³digo balanceado | code-davinci-002 | qwen2.5-coder:14b | code |
| CÃ³digo premium | gpt-4-code | qwen3-coder:30b | code |

---

## ðŸš€ IMPLEMENTACIÃ“N

Para implementar este mapeo mejorado, se debe actualizar el archivo:
`shared/default_compat_models.py`

Con los cambios mencionados arriba, especialmente:
1. Cambiar los modelos indicados
2. Agregar los 5 nuevos modelos
3. Actualizar las definiciones de `litellm_params` y `model_info` con las caracterÃ­sticas correctas
