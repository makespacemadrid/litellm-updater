services:
  # Backend sync worker (no HTTP server)
  litellm-companion-backend:
    image: ghcr.io/makespacemadrid/litellm-companion-backend:latest
    build:
      context: .
      dockerfile: Dockerfile
    command: bash /app/scripts/start_backend.sh
    container_name: litellm-companion-backend
    restart: unless-stopped
    volumes:
      - ./data:/app/data
    environment:
      - LOG_LEVEL=${LOG_LEVEL:-info}
    networks:
      - litellm
    labels:
      com.centurylinklabs.watchtower.enable: "true"

  # Frontend API + UI
  litellm-companion-web:
    image: ghcr.io/makespacemadrid/litellm-companion-web:latest
    build:
      context: .
      dockerfile: Dockerfile
    command: bash /app/scripts/start_web.sh
    container_name: litellm-companion-web
    restart: unless-stopped
    ports:
      - "${PORT:-4001}:8000"
    volumes:
      - ./data:/app/data
    environment:
      - LOG_LEVEL=${LOG_LEVEL:-info}
    depends_on:
      - litellm-companion-backend
    networks:
      - litellm
    labels:
      com.centurylinklabs.watchtower.enable: "true"

  # OpenAI-compatible proxy (chat/completions)
  litellm-companion-proxy:
    image: ghcr.io/makespacemadrid/litellm-companion-proxy:latest
    build:
      context: .
      dockerfile: Dockerfile
    command: uvicorn proxy.api:create_app --factory --host 0.0.0.0 --port 8000
    container_name: litellm-companion-proxy
    restart: unless-stopped
    ports:
      - "${PROXY_PORT:-4002}:8000"
    volumes:
      - ./data:/app/data
    environment:
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - PROXY_LOG_REQUESTS=${PROXY_LOG_REQUESTS:-false}
      - PROXY_LOG_BODY=${PROXY_LOG_BODY:-false}
    networks:
      - litellm
    labels:
      com.centurylinklabs.watchtower.enable: "true"

  litellm:
    image: ghcr.io/berriai/litellm:main-stable
    ports:
      - "4000:4000"
    env_file:
      - .env
    depends_on:
      db:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget --no-verbose --tries=1 http://localhost:4000/health/liveliness || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    networks:
      - litellm
  db:
    image: postgres:16
    restart: always
    container_name: litellm_db
    env_file:
      - .env
#    ports:
#      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -d ${POSTGRES_DB} -U ${POSTGRES_USER}"]
      interval: 1s
      timeout: 5s
      retries: 10
    networks:
      - litellm
#  env-sync:
#    build: .
#    image: litellm-companion:latest
#    command: >
#      python /app/scripts/sync_env.py
#    volumes:
#      - ./.env:/app/.env
#    environment:
#      EXAMPLE_ENV_PATH: /app/env.example
#      TARGET_ENV_PATH: /app/.env
#    labels:
#      com.centurylinklabs.watchtower.enable: "false"

  watchtower:
    image: containrrr/watchtower:latest
    command: --interval ${WATCHTOWER_POLL_INTERVAL:-60} --label-enable
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
#    depends_on:
#      - litellm-companion

networks:
  litellm:
volumes:
  postgres_data:
