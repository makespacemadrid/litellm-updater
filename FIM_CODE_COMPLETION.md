# Fill-in-the-Middle (FIM) / Code Completion con LiteLLM

## üìã Resumen

LiteLLM Companion detecta autom√°ticamente modelos con capacidad FIM (Fill-in-the-Middle) y marca esta capacidad en los metadatos del modelo. Los clientes como Continue.dev y Cursor pueden usar los modelos directamente sin necesidad de configuraci√≥n especial.

## ‚úÖ Estado Actual en Nuestro Sistema

### 1. Detecci√≥n Autom√°tica

```python
# Los modelos qwen2.5-coder reportan desde Ollama:
{
  "capabilities": ["completion", "tools", "insert"]
}

# Nuestro c√≥digo (shared/models.py) autom√°ticamente:
# - Extrae la capability "insert" / "fill_in_middle" / "fim"
# - La convierte a campos: supports_fill_in_middle, supports_code_infilling
# - Los marca en model_info al registrar en LiteLLM
```

### 2. Modelos con FIM Detectados

- `qwen2.5-coder:1.5b` ‚úì
- `qwen2.5-coder:7b` ‚úì
- `qwen2.5-coder:7b-base` ‚úì
- `qwen2.5-coder:14b` ‚úì
- Otros modelos que reporten "insert" en capabilities

## üîß C√≥mo Funciona

### Registro en LiteLLM

Cuando un proveedor tiene `auto_detect_fim=true` (por defecto), el sistema:

1. **Detecta** modelos con `supports_fill_in_middle` en sus capacidades
2. **Marca** en `model_info`:
   ```json
   {
     "supports_fill_in_middle": true,
     "supports_code_infilling": true
   }
   ```
3. **Registra** el modelo normalmente en LiteLLM (sin duplicados ni prefijos especiales)

### Ejemplo de Modelo Registrado

```json
{
  "model_name": "mks-ollama/qwen2.5-coder:7b",
  "litellm_params": {
    "model": "ollama_chat/qwen2.5-coder:7b",
    "api_base": "http://ollama:11434",
    "tags": ["capability:fill-in-middle", "capability:code-infilling", ...]
  },
  "model_info": {
    "litellm_provider": "ollama",
    "mode": "ollama_chat",
    "supports_fill_in_middle": true,
    "supports_code_infilling": true
  }
}
```

## üéØ Integraci√≥n con Continue.dev / Cursor

### Continue.dev

**Configuraci√≥n para Tab Autocomplete:**

```json
{
  "tabAutocompleteModel": {
    "provider": "siliconflow",
    "model": "qwen2.5-coder:7b",
    "apiBase": "http://localhost:4000/",
    "apiKey": "sk-1234"
  }
}
```

**¬øPor qu√© "siliconflow"?**
- Continue.dev usa formato FIM nativo cuando detecta ciertos providers
- `siliconflow` es uno de los providers que soporta FIM autom√°ticamente
- Esto evita que Continue.dev aplique formato chat a las peticiones

**Alternativa (si el provider soporta OpenAI + FIM):**

```json
{
  "tabAutocompleteModel": {
    "provider": "openai",
    "model": "qwen2.5-coder:7b",
    "apiBase": "http://localhost:4000/v1",
    "apiKey": "sk-1234"
  }
}
```

### Cursor

Similar configuraci√≥n en `settings.json`:

```json
{
  "cursor.cpp.fimModel": {
    "provider": "siliconflow",
    "model": "qwen2.5-coder:7b",
    "apiBase": "http://localhost:4000/",
    "apiKey": "sk-1234"
  }
}
```

## üìä Ventajas del Nuevo Enfoque

| Aspecto | Enfoque Anterior | Nuevo Enfoque |
|---------|------------------|---------------|
| **Modelos duplicados** | ‚úó Requer√≠a versi√≥n -fim separada | ‚úì Un solo modelo |
| **Configuraci√≥n** | ‚úó Prefijo text-completion-codestral | ‚úì Modo normal |
| **Detecci√≥n FIM** | ‚úì Autom√°tica | ‚úì Autom√°tica |
| **Metadatos** | ‚ö†Ô∏è En modelo separado | ‚úì En modelo principal |
| **Simplicidad** | ‚úó Compleja | ‚úì Simple |
| **Mantenimiento** | ‚úó Dos modelos | ‚úì Un modelo |

## üîç Verificaci√≥n

### Check si un modelo tiene FIM:

```bash
# Via LiteLLM API
curl http://localhost:4000/model/info \
  -H "Authorization: Bearer sk-1234" | \
  jq '.data[] | select(.model_info.supports_fill_in_middle == true) | .model_name'

# Via API local
curl http://localhost:8000/api/models/123 | \
  jq '{
    model: .model_id,
    fim: .litellm_params.supports_fill_in_middle,
    infilling: .litellm_params.supports_code_infilling
  }'
```

### Test FIM directo con Ollama:

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5-coder:7b",
  "prompt": "def fibonacci(n):",
  "suffix": "    return result",
  "stream": false
}' | jq -r '.response'
```

### Test via LiteLLM con Continue.dev:

1. Configura Continue.dev con `provider: "siliconflow"`
2. El autocompletado deber√≠a funcionar autom√°ticamente
3. LiteLLM rutea la petici√≥n a Ollama preservando el contexto FIM

## ‚öôÔ∏è Configuraci√≥n del Provider

### Habilitar/Deshabilitar Auto-detecci√≥n FIM

En la UI de Admin (`/admin`), al editar un provider:

```
‚òë Auto-detect FIM
  Automatically detect and mark Fill-in-the-Middle capability
  for code models with insert/infilling support
```

Por defecto est√° **habilitado**. Si lo deshabilitas:
- No se detectar√° FIM autom√°ticamente
- Puedes marcar manualmente `supports_fill_in_middle` en los par√°metros del modelo

## üìö Referencias

- [Continue.dev Issue #4542](https://github.com/continuedev/continue/issues/4542) - Tab autocomplete configuration
- [LiteLLM Issue #9251](https://github.com/BerriAI/litellm/issues/9251) - FIM/Completions support
- [Ollama API - Generate](https://docs.ollama.com/api/generate) - Documentaci√≥n del par√°metro suffix
- [Ollama FIM Issue #3869](https://github.com/ollama/ollama/issues/3869) - FIM API support

## üé¨ Estado de Implementaci√≥n

1. ‚úÖ **HECHO:** Detecci√≥n de capability "insert" ‚Üí `supports_fill_in_middle`
2. ‚úÖ **HECHO:** Marcado autom√°tico de capacidad FIM en model_info
3. ‚úÖ **HECHO:** Eliminado workaround de text-completion-codestral
4. ‚úÖ **HECHO:** Simplificado a un solo modelo por versi√≥n
5. ‚è≥ **PENDIENTE:** Documentar configuraci√≥n espec√≠fica para otros IDEs
6. ‚è≥ **PENDIENTE:** Tests de integraci√≥n con Continue.dev/Cursor

---

**√öltima actualizaci√≥n:** 2025-12-26
**Estado:** Sistema simplificado - FIM detectado autom√°ticamente como capacidad del modelo
