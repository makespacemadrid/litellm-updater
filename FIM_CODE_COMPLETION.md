# Fill-in-the-Middle (FIM) / Code Completion con LiteLLM

## üìã Resumen

Basado en los issues [#4542](https://github.com/continuedev/continue/issues/4542) y [#9251](https://github.com/BerriAI/litellm/issues/9251), **S√ç es posible** usar FIM (Fill-in-the-Middle) a trav√©s de LiteLLM, pero requiere configuraci√≥n especial.

## ‚úÖ Estado Actual en Nuestro Sistema

### 1. Detecci√≥n Autom√°tica
```python
# Los modelos qwen2.5-coder reportan desde Ollama:
{
  "capabilities": ["completion", "tools", "insert"]
}

# Nuestro c√≥digo (shared/models.py) autom√°ticamente:
# - Extrae la capability "insert"
# - La convierte a tag: "capability:insert"
# - Mapea a campos: supports_fill_in_middle, supports_code_infilling
```

### 2. Modelos con FIM Detectados
- `qwen2.5-coder:1.5b` ‚úì
- `qwen2.5-coder:7b` ‚úì
- `qwen2.5-coder:7b-base` ‚úì
- `qwen2.5-coder:14b` ‚úì

## üîß C√≥mo Funciona FIM

### Opci√≥n A: Usar Ollama Directo (M√°s Simple)

**Endpoint:** `/api/generate`

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5-coder:7b",
  "prompt": "def compute_gcd(a, b):",
  "suffix": "    return result",
  "stream": false
}'
```

**Ventajas:**
- ‚úì Funciona directamente sin configuraci√≥n adicional
- ‚úì Formato nativo de Ollama
- ‚úì Soporta `suffix` parameter

**Desventajas:**
- ‚úó No pasa por LiteLLM (sin logging/analytics/rate limiting)
- ‚úó No puede usar access groups de LiteLLM
- ‚úó No aparece en el dashboard de LiteLLM

### Opci√≥n B: A trav√©s de LiteLLM (Requiere Workaround)

**Endpoint:** `/v1/completions` (NO `/fim/completions`)

**IMPORTANTE:** LiteLLM no tiene endpoint nativo `/fim/completions`, pero podemos usar `/v1/completions` con un truco.

#### Paso 1: Registrar el Modelo con Prefijo Especial

```python
# En LiteLLM, registrar con prefijo text-completion-codestral/
{
  "model_name": "mks-ollama/qwen2.5-coder:7b-fim",
  "litellm_params": {
    "model": "text-completion-codestral/qwen2.5-coder:7b",  # ‚Üê Prefijo m√°gico
    "api_base": "http://ollama:11434"
  },
  "model_info": {
    "mode": "completion",  # NO "chat"
    "supports_fill_in_middle": true
  }
}
```

#### Paso 2: Llamar al Endpoint `/v1/completions`

```bash
curl http://localhost:4000/v1/completions \
  -H "Authorization: Bearer sk-1234" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "mks-ollama/qwen2.5-coder:7b-fim",
    "prompt": "def compute_gcd(a, b):",
    "suffix": "    return result",
    "max_tokens": 100,
    "temperature": 0
  }'
```

**Ventajas:**
- ‚úì Pasa por LiteLLM (logging, analytics, rate limiting)
- ‚úì Access groups de LiteLLM
- ‚úì Dashboard unificado

**Desventajas:**
- ‚úó Requiere registrar modelo DOS veces (versi√≥n chat + versi√≥n FIM)
- ‚úó Workaround no oficial (puede cambiar)
- ‚úó M√°s complejo de configurar

## üéØ Integraci√≥n con Continue.dev / Cursor

### Continue.dev

**Configuraci√≥n para Tab Autocomplete:**

```json
{
  "tabAutocompleteModel": {
    "provider": "siliconflow",  // ‚Üê Trick: NO usar "openai"
    "model": "qwen2.5-coder:7b-fim",
    "apiBase": "http://localhost:4000/",
    "apiKey": "sk-1234"
  }
}
```

**¬øPor qu√© "siliconflow"?**
- Continue.dev aplica formato chat cuando detecta provider "openai"
- Usando "siliconflow" env√≠a el formato correcto con tokens FIM

### Cursor / Otros IDEs

Similar configuraci√≥n, depende de c√≥mo el IDE env√≠a las peticiones.

## üìä Comparativa de Enfoques

| Aspecto | Ollama Directo | LiteLLM Proxy |
|---------|----------------|---------------|
| **Endpoint** | `/api/generate` | `/v1/completions` |
| **Configuraci√≥n** | Simple | Compleja (prefijo especial) |
| **Par√°metro suffix** | ‚úì Nativo | ‚úì Via workaround |
| **LiteLLM logging** | ‚úó | ‚úì |
| **Access control** | ‚úó | ‚úì |
| **Dashboard** | ‚úó | ‚úì |
| **Rate limiting** | ‚úó | ‚úì |
| **Estabilidad** | ‚úì‚úì‚úì | ‚ö†Ô∏è Workaround |

## üöÄ Propuesta de Implementaci√≥n

### Opci√≥n 1: Doble Registro (Autom√°tico)

Para cada modelo con `capability:insert`, registrar DOS versiones en LiteLLM:

```python
# Ejemplo: qwen2.5-coder:7b

# 1. Versi√≥n Chat (normal)
{
  "model_name": "mks-ollama/qwen2.5-coder:7b",
  "litellm_params": {
    "model": "ollama/qwen2.5-coder:7b",
    "api_base": "http://ollama:11434"
  },
  "model_info": {
    "mode": "chat"
  }
}

# 2. Versi√≥n FIM (code completion)
{
  "model_name": "mks-ollama/qwen2.5-coder:7b-fim",  # Sufijo -fim
  "litellm_params": {
    "model": "text-completion-codestral/qwen2.5-coder:7b",  # Prefijo especial
    "api_base": "http://ollama:11434"
  },
  "model_info": {
    "mode": "completion",
    "supports_fill_in_middle": true,
    "supports_code_infilling": true
  }
}
```

### Opci√≥n 2: Solo Documentar (Manual)

- Documentar que los usuarios pueden usar Ollama directo para FIM
- Proporcionar ejemplos de configuraci√≥n para Continue.dev/Cursor
- No registrar autom√°ticamente versiones FIM

### Opci√≥n 3: Flag de Usuario

Agregar un flag en la UI del provider:
```
‚òê Register FIM variants for code models
```

Si est√° activado, auto-registrar versiones `-fim` de modelos con `capability:insert`.

## üîç Verificaci√≥n

### Check si un modelo tiene FIM:

```bash
# Via API
curl http://localhost:8000/api/models/123 | jq '.litellm_params.supports_fill_in_middle'

# Via database
sqlite3 data/models.db "
  SELECT model_id, litellm_params
  FROM models m
  JOIN providers p ON m.provider_id = p.id
  WHERE p.name = 'mks'
    AND json_extract(litellm_params, '$.supports_fill_in_middle') = 1
"
```

### Test FIM directo con Ollama:

```bash
curl http://localhost:11434/api/generate -d '{
  "model": "qwen2.5-coder:7b",
  "prompt": "def fibonacci(n):",
  "suffix": "    return result",
  "stream": false
}' | jq -r '.response'
```

### Test FIM via LiteLLM:

```bash
# Primero registrar el modelo con prefijo text-completion-codestral/
# Luego:
curl http://localhost:4000/v1/completions \
  -H "Authorization: Bearer sk-1234" \
  -d '{
    "model": "mks-ollama/qwen2.5-coder:7b-fim",
    "prompt": "def fibonacci(n):",
    "suffix": "    return result",
    "max_tokens": 100
  }' | jq -r '.choices[0].text'
```

## üìö Referencias

- [Continue.dev Issue #4542](https://github.com/continuedev/continue/issues/4542) - Tab autocomplete no funciona con OpenAI provider
- [LiteLLM Issue #9251](https://github.com/BerriAI/litellm/issues/9251) - FIM/Completions support
- [Ollama API - Generate](https://docs.ollama.com/api/generate) - Documentaci√≥n oficial del par√°metro suffix
- [Ollama FIM Issue #3869](https://github.com/ollama/ollama/issues/3869) - API para FIM tasks

## üé¨ Pr√≥ximos Pasos

1. ‚úÖ **HECHO:** Agregar detecci√≥n de capability "insert" ‚Üí `supports_fill_in_middle`
2. ‚è≥ **PENDIENTE:** Decidir estrategia de registro (autom√°tico vs manual)
3. ‚è≥ **PENDIENTE:** Actualizar UI para mostrar modelos con FIM
4. ‚è≥ **PENDIENTE:** Documentar configuraci√≥n para Continue.dev/Cursor
5. ‚è≥ **PENDIENTE:** Tests de integraci√≥n con FIM

---

**√öltima actualizaci√≥n:** 2025-12-25
**Estado:** Campo `supports_fill_in_middle` implementado, pendiente estrategia de registro dual
